{
  "xml": {
    "feed": {
      "$": {
        "xmlns": "http://www.w3.org/2005/Atom"
      },
      "link": [
        {
          "$": {
            "href": "http://arxiv.org/api/query?search_query%3D%26id_list%3D2506.19697%26start%3D0%26max_results%3D10",
            "rel": "self",
            "type": "application/atom+xml"
          }
        }
      ],
      "title": [
        {
          "_": "ArXiv Query: search_query=&id_list=2506.19697&start=0&max_results=10",
          "$": {
            "type": "html"
          }
        }
      ],
      "id": ["http://arxiv.org/api/cNdvGVegoXv1OMsDbxnXG5ZZ9CE"],
      "updated": ["2025-07-01T00:00:00-04:00"],
      "opensearch:totalResults": [
        {
          "_": "1",
          "$": {
            "xmlns:opensearch": "http://a9.com/-/spec/opensearch/1.1/"
          }
        }
      ],
      "opensearch:startIndex": [
        {
          "_": "0",
          "$": {
            "xmlns:opensearch": "http://a9.com/-/spec/opensearch/1.1/"
          }
        }
      ],
      "opensearch:itemsPerPage": [
        {
          "_": "10",
          "$": {
            "xmlns:opensearch": "http://a9.com/-/spec/opensearch/1.1/"
          }
        }
      ],
      "entry": [
        {
          "id": ["http://arxiv.org/abs/2506.19697v1"],
          "updated": ["2025-06-24T15:03:57Z"],
          "published": ["2025-06-24T15:03:57Z"],
          "title": [
            "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models"
          ],
          "summary": [
            "  Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.\n"
          ],
          "author": [
            {
              "name": ["Jungwoo Park"]
            },
            {
              "name": ["Taewhoo Lee"]
            },
            {
              "name": ["Chanwoong Yoon"]
            },
            {
              "name": ["Hyeon Hwang"]
            },
            {
              "name": ["Jaewoo Kang"]
            }
          ],
          "link": [
            {
              "$": {
                "href": "http://arxiv.org/abs/2506.19697v1",
                "rel": "alternate",
                "type": "text/html"
              }
            },
            {
              "$": {
                "title": "pdf",
                "href": "http://arxiv.org/pdf/2506.19697v1",
                "rel": "related",
                "type": "application/pdf"
              }
            }
          ],
          "arxiv:primary_category": [
            {
              "$": {
                "xmlns:arxiv": "http://arxiv.org/schemas/atom",
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom"
              }
            }
          ],
          "category": [
            {
              "$": {
                "term": "cs.LG",
                "scheme": "http://arxiv.org/schemas/atom"
              }
            },
            {
              "$": {
                "term": "cs.AI",
                "scheme": "http://arxiv.org/schemas/atom"
              }
            },
            {
              "$": {
                "term": "cs.CL",
                "scheme": "http://arxiv.org/schemas/atom"
              }
            }
          ]
        }
      ]
    }
  }
}
